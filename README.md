# LLM CLI Agent with MCP Integration

A sophisticated command-line AI assistant that integrates LLMs (OpenAI/Anthropic) with your MCP (Model Context Protocol) server for intelligent tool usage.

## ğŸ—ï¸ Architecture

```
[CLI Interface] â†’ [LLM Agent] â†’ [LLM Client] â†’ [OpenAI/Anthropic API]
                      â†“
                 [MCP Orchestrator] â†’ [SSE Client] â†’ [Your MCP Server] â†’ [Tools]
```

## ğŸš€ Quick Start

### 1. Start Your MCP Server
```bash
# In one terminal, start your MCP server
cd mcp-server
python3 server_sse.py
```

### 2. Set API Key
```bash
# Set your OpenAI API key
export OPENAI_API_KEY="your-openai-api-key-here"

# Or for Anthropic
export ANTHROPIC_API_KEY="your-anthropic-api-key-here"
export LLM_PROVIDER="anthropic"
```

### 3. Start the Agent
```bash
# In another terminal, start the CLI agent
cd mcp-client
python3 start_agent.py
```

## ğŸ’¬ Usage Examples

### Basic Chat
```
ğŸ’¬ You: Hello! What can you help me with?
ğŸ¤– Assistant: I'm an AI assistant with access to various tools via MCP...
```

### File Operations
```
ğŸ’¬ You: List the files in the workspace directory
ğŸ¤– Assistant: I'll check the workspace directory for you.

[Tool: file_ops executes]

ğŸ¤– Assistant: I found 3 items in the workspace:
- ğŸ“„ test.txt
- ğŸ example.py  
- ğŸ“ data/
```

### Code Analysis
```
ğŸ’¬ You: Read and analyze the example.py file
ğŸ¤– Assistant: Let me read the file and analyze it for you.

[Tool execution shows code content and analysis]
```

### Multi-Tool Workflows
```
ğŸ’¬ You: Create a new file called "hello.txt" with the content "Hello MCP!" and then read it back
ğŸ¤– Assistant: I'll create the file and then read it back to confirm.

[Executes: write file â†’ read file]
```

## ğŸ›ï¸ CLI Commands

| Command | Description |
|---------|-------------|
| `/help` | Show help information |
| `/status` | Show agent and MCP server status |
| `/tools` | List available MCP tools |
| `/clear` | Clear conversation history |
| `/switch` | Switch LLM provider/model |
| `/debug` | Toggle debug logging |
| `/exit` | Exit the agent |

## âš™ï¸ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_API_KEY` | OpenAI API key | Required for OpenAI |
| `ANTHROPIC_API_KEY` | Anthropic API key | Required for Anthropic |
| `LLM_PROVIDER` | Provider: "openai" or "anthropic" | "openai" |
| `OPENAI_MODEL` | OpenAI model name | "gpt-4" |
| `ANTHROPIC_MODEL` | Anthropic model name | "claude-3-sonnet-20240229" |
| `MCP_BASE_URL` | MCP server URL | "http://localhost:8081" |
| `LLM_TEMPERATURE` | LLM temperature | "0.7" |
| `DEBUG` | Enable debug mode | "false" |

### Provider Switching

#### Use OpenAI GPT-4
```bash
export LLM_PROVIDER="openai"
export OPENAI_MODEL="gpt-4"
export OPENAI_API_KEY="your-key"
```

#### Use Anthropic Claude
```bash
export LLM_PROVIDER="anthropic" 
export ANTHROPIC_MODEL="claude-3-sonnet-20240229"
export ANTHROPIC_API_KEY="your-key"
```

#### Use OpenAI GPT-3.5 (faster/cheaper)
```bash
export LLM_PROVIDER="openai"
export OPENAI_MODEL="gpt-3.5-turbo"
```

## ğŸ”§ Available Models

### OpenAI Models
- `gpt-4` - Most capable
- `gpt-4-turbo-preview` - Faster GPT-4
- `gpt-3.5-turbo` - Fast and economical

### Anthropic Models  
- `claude-3-opus-20240229` - Most capable
- `claude-3-sonnet-20240229` - Balanced performance
- `claude-3-haiku-20240307` - Fast and economical

## ğŸ§  How It Works

### 1. Tool Discovery
The agent automatically discovers available tools from your MCP server via the `/tools` endpoint.

### 2. Intelligent Tool Selection
The LLM analyzes user requests and determines which tools to use, including:
- Parameter extraction from natural language
- Multi-step tool sequences
- Error handling and retries

### 3. Tool Execution
Tools are executed via the SSE (Server-Sent Events) transport for real-time streaming.

### 4. Result Integration
Tool results are formatted and integrated into the conversation context for follow-up questions.

## ğŸ” Example Interactions

### File Management
```
ğŸ’¬ You: Show me what's in the workspace, then create a new file called notes.md
ğŸ¤– Assistant: I'll check the workspace contents first, then create the notes file.

[Executes file_ops with operation=list]
[Executes file_ops with operation=write]

ğŸ¤– Assistant: The workspace contains 3 files... I've created notes.md successfully!
```

### Tool Chaining
```
ğŸ’¬ You: Read the config file and tell me what settings are defined
ğŸ¤– Assistant: Let me read the configuration file and analyze its contents.

[Reads file â†’ Analyzes content â†’ Provides summary]
```

## ğŸ› ï¸ Development

### Project Structure
```
mcp-client/
â”œâ”€â”€ llm_agent/
â”‚   â”œâ”€â”€ __init__.py          # Package initialization
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ llm_client.py        # LLM abstraction layer
â”‚   â”œâ”€â”€ mcp_orchestrator.py  # MCP integration
â”‚   â””â”€â”€ cli_agent.py         # CLI interface
â”œâ”€â”€ start_agent.py           # Launcher script
â””â”€â”€ README.md               # This file
```

### Key Components

1. **LLM Client** (`llm_client.py`)
   - Provider-agnostic interface
   - OpenAI and Anthropic support
   - Streaming and completion APIs

2. **MCP Orchestrator** (`mcp_orchestrator.py`)
   - Tool discovery and execution
   - LLM â†” MCP integration
   - Context management

3. **CLI Agent** (`cli_agent.py`)
   - Interactive command interface
   - Session management
   - User experience

## ğŸ§ª Testing

### Test LLM Connection
```python
from llm_agent.config import ConfigManager
from llm_agent.llm_client import LLMClientFactory

config = ConfigManager.load_from_env()
client = LLMClientFactory.create_client(config.llm)
response = await client.simple_complete("Say hello!")
print(response)
```

### Test MCP Integration
```python
from llm_agent.mcp_orchestrator import MCPOrchestrator

orchestrator = MCPOrchestrator(llm_client, mcp_config)
async with orchestrator.session():
    tools = await orchestrator.discover_tools()
    print(f"Found {len(tools)} tools")
```

## ğŸ› Troubleshooting

### Agent Won't Start
- Check API key environment variables
- Verify MCP server is running on http://localhost:8081
- Check network connectivity

### Tools Not Found
- Ensure MCP server is running: `python3 server_sse.py`
- Check server logs for tool registration
- Verify workspace directory has test files

### LLM Errors
- Verify API key is correct
- Check rate limits and quotas
- Try switching to a different model

### Debug Mode
```bash
export DEBUG=true
python3 start_agent.py
```

## ğŸ¯ Next Steps

1. **Add more tools** to your MCP server
2. **Customize prompts** for specific use cases  
3. **Implement tool chaining** for complex workflows
4. **Add memory/persistence** for longer conversations
5. **Create specialized agents** for different domains

## ğŸ“ License

This implementation follows the BÃºvÃ¡r architectural patterns with:
- âœ… Dependency injection
- âœ… Context management  
- âœ… Async-first design
- âœ… Performance optimization
- âœ… Clean separation of concerns
